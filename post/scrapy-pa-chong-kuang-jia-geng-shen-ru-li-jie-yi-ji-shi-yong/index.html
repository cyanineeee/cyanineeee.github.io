<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>scrapy爬虫框架深入理解 -- 未整理完 | cyanine</title>
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico?v=1723632390821">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="http://localhost:4000/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="scrapy爬虫框架深入理解 -- 未整理完 | cyanine - Atom Feed" href="http://localhost:4000/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="1. logging模块
2. 环境变量（setting）
官方文档
2.1 如何设置并访问setting
2.1.1 在spider中
spider中，setting是一个继承scrapy.Spider的一个实例属性，可以通过self.s..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="http://localhost:4000">
  <img class="avatar" src="http://localhost:4000/images/avatar.png?v=1723632390821" alt="">
  </a>
  <h1 class="site-title">
    cyanine
  </h1>
  <p class="site-description">
    my personal blog
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          Home Page
        </a>
      
    
      
        <a href="/archives" class="menu">
          Archive
        </a>
      
    
      
        <a href="/tags" class="menu">
          Tags
        </a>
      
    
      
        <a href="/post/about" class="menu">
          About
        </a>
      
    
      
        <a href="http://localhost:4000/post/cczhi-zhen" class="menu">
          凑数
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              scrapy爬虫框架深入理解 -- 未整理完
            </h2>
            <div class="post-info">
              <span>
                2023-06-17
              </span>
              <span>
                6 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <h2 id="1-logging模块">1. logging模块</h2>
<h2 id="2-环境变量setting">2. 环境变量（setting）</h2>
<p><a href="https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings">官方文档</a></p>
<h3 id="21-如何设置并访问setting">2.1 如何设置并访问setting</h3>
<h4 id="211-在spider中">2.1.1 在spider中</h4>
<p>spider中，setting是一个继承scrapy.Spider的一个实例属性，可以通过self.setting访问对应在setting.py中的环境变量。</p>
<pre><code class="language-python">#在setting.py文件中
MONGO_URI=' 对应的URI'
</code></pre>
<pre><code class="language-python">#在spider中调用
class ExampleSpider(scrapy.Spider):
    ...#省略
    def parse(self, response):
        mongodb = self.settings['MONGO_URI']
        logging.log(logging.INFO,mongodb)
    #会在日志中输出
    #2023-06-17 16:44:04 [root] INFO: 对应的URI
</code></pre>
<p>scrapy同样支持为每一个Spider设置不同的环境变量，以避免所有环境变量都挤在一个setting.py文件中造成臃肿。在每一个spider实例中都有一个继承来的custom_settings属性可以修改，它是一个字典类型，所有单独的环境变量都可以在这里声明，包括cookies, header, 以及其他可以写在setting.py文件中的变量。声明方法跟Spider的name属性相同。调用则通过self.custom_settings使用在方法中。</p>
<h4 id="212-在pipeline等其他组件中">2.1.2 在pipeline等其他组件中</h4>
<p>使用类方法from_crawler()可以访问到<br>
<a href="https://docs.scrapy.org/en/latest/topics/settings.html#how-to-access-settings">官方文档参考</a></p>
<blockquote>
<p>“<strong>Settings can be accessed through the scrapy.crawler.Crawler.settings attribute of the Crawler that is passed to from_crawler method in extensions, middlewares and item pipelines</strong>”</p>
</blockquote>
<p>示例代码：</p>
<pre><code class="language-python">class MyExtension:
    def __init__(self, log_is_enabled=False):
        if log_is_enabled:
            print(&quot;log is enabled!&quot;)

    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        return cls(settings.getbool(&quot;LOG_ENABLED&quot;))
</code></pre>
<p>在<code>from_crawler()</code>方法中的setting对象是一个类字典对象，因此可以用字典的方式访问环境变量(例如：<code>e.g., settings['LOG_ENABLED']</code>)，但是为了避免可能的类型错误（比如调用需要int类型，但是环境变量中是str类型，直接使用key访问就可能返回int类型造成错误），更推荐使用setting对象的api。</p>
<blockquote>
<p><a href="https://docs.scrapy.org/en/latest/topics/api.html#scrapy.settings.Settings">官方文档中setting对象的api</a><br>
包括<code>get()</code>,<code>getbool()</code>, <code>getdict()</code>, <code>getdictorlist()</code>, <code>getfloat()</code>,  <code>getint()</code>,  <code>getlist()</code>等，用途也能从方法名显而易见出，具体请参考官方链接。</p>
</blockquote>
<h2 id="1-crawler模块">1. crawler模块</h2>
<h2 id="2-item源码如何实现存储key字段">2. item源码如何实现存储key字段,</h2>
<p>https://www.cnblogs.com/twelfthing/articles/4709287.html<br>
https://www.cnblogs.com/fengf233/p/11298623.html#2.field()%E7%B1%BB</p>
<h2 id="5-多个spider以及多个pipeline对应的设置">5. 多个Spider以及多个Pipeline对应的设置</h2>
<p>https://www.ziji.work/python/scrapy-many-spider-pipeline.html#SCRAPYSPIDER-4</p>
<h2 id="6-大佬的源码解析">6. 大佬的源码解析</h2>
<p>虽然现在基本看不懂（×<br>
http://kaito-kidd.com/2016/11/09/scrapy-code-analyze-entrance/</p>
<h2 id="7-访问setting">7. 访问setting</h2>
<h2 id="8-为什么from_crawler方法需要设置为类方法">8. 为什么from_crawler方法需要设置为类方法？</h2>
<p><a href="https://www.keepnight.com/archives/1126/">提点链接</a><br>
最后这篇博客的最后一句话简直一下子给我点透了，虽然之前专门去查了<code>@classmethod</code>、factory method这些东西，但为什么这么做还是很混乱。</p>
<blockquote>
<p><strong>大概就是检测spider类有没有from_crawler，有的话就return一个cls()的实例化对象，产生实例化对象后会自动调__init__方法。</strong></p>
</blockquote>
<p>结合官方文档的<a href="https://docs.scrapy.org/en/latest/topics/api.html?highlight=crawler#scrapy.crawler.CrawlerRunner.create_crawler">Crawler API</a>、<a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html#from_crawler">pipeline中from_crawler()方法</a>、<a href="https://docs.scrapy.org/en/latest/topics/spiders.html?highlight=crawler#scrapy.Spider.from_crawler">Spider中from_crawler()方法</a>，以及<a href="http://kaito-kidd.com/2016/11/09/scrapy-code-analyze-entrance/">大佬对scrapy源码的解析</a>才有一种突然醒悟的感觉。</p>
<blockquote>
<p>PS. 这里我一直犯了一个概念上的错误，我一直以为示例代码中<code>from_crawler(cls, cralwer)</code> 的crawler是类的属性参数（我误认为是有点像构造方法中的参数，被我搞混了😣），是一个定义在当前类里面的一个继承过来的属性，因此花了很多时间去查这个<code>crawler</code>在scrapy中的作用。但是实际上它只是一个函数的形参，它本身不带有任何意义（就像是定义函数的时候括号里的a,b,c,d等参数一样）。然后又因为crawler在scrapy又是一个关键对象，所以查了很久都没有结果。<br>
直到我看到了上面那篇提点链接的博客，这才想通了。</p>
</blockquote>
<p>从我的理解来说 ：<br>
第一步，在运行这个爬虫之前，scrapy会为运行做一些准备，其中就包括判断pipeline， spider, 还有其他一些组件中的<code>from_crawler()</code>方法是否被重写，这个判断就需要在类没有实例化之前调用，所以<code>from_crawler()</code>被定义为类方法。<br>
第二步，之后根据具体的类，在从Crawler这个对象中实例化一个对应的from_crawler方法。<br>
上面这句话有两点需要着重理解。</p>
<ol>
<li>第一点，这也就是<code>from_crawler()</code>是一个“factory method”，scrapy会根据不同的对象为类赋予不同的<code>from_crawler()</code>方法。在我的理解中，就是有一个类，专门负责为其他不同的类赋予不同的<code>from_crawler()</code>方法，也就是<strong>类方法的类</strong>。</li>
<li>第二点，scrapy从Crawler<code>创建from_crawler()</code>的含义就是为这个方法传入一个<code>scrapy.crawler.Crawler</code>对象，而这个对象又是由scrapy通过一个<code>Spider子类</code>和<code>scrapy.settings.Settings </code>对象实例化而来。</li>
</ol>
<blockquote>
<p><a href="https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler">官方文档</a><br>
<strong><code>classscrapy.crawler.Crawler(spidercls, settings)</code></strong><br>
<strong>The Crawler object must be instantiated with a scrapy.Spider subclass and a scrapy.settings.Settings object.</strong></p>
</blockquote>
<p>因此，在Crawler对象中就有能过获取setting的API，这个API和2.1.2中的setting对象的一模一样（因为本来就是一个东西👏，都是setting对象，这里就有串起来的感觉了）</p>
<h3 id="9-设置cookies">9. 设置cookies</h3>
<p><a href="https://zhuanlan.zhihu.com/p/337212121">知乎参考</a>但是没有讲原理</p>
<h3 id="10-关闭scrapy日志">10. 关闭scrapy日志</h3>
<p><a href="https://www.qiniu.com/qfans/qnso-33203620">参考</a><br>
<a href="https://coding.imooc.com/learn/questiondetail/133488.html">解决方法</a></p>
<p>添加twisted定时器 以及 输出调用日志<br>
https://www.jianshu.com/p/5a5cdd7f2bfb</p>
<h3 id="11-定时调度scrapy">11. 定时调度scrapy</h3>
<p>https://blog.csdn.net/python36/article/details/82683528</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#1-logging%E6%A8%A1%E5%9D%97">1. logging模块</a></li>
<li><a href="#2-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8Fsetting">2. 环境变量（setting）</a>
<ul>
<li><a href="#21-%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E5%B9%B6%E8%AE%BF%E9%97%AEsetting">2.1 如何设置并访问setting</a>
<ul>
<li><a href="#211-%E5%9C%A8spider%E4%B8%AD">2.1.1 在spider中</a></li>
<li><a href="#212-%E5%9C%A8pipeline%E7%AD%89%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6%E4%B8%AD">2.1.2 在pipeline等其他组件中</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#1-crawler%E6%A8%A1%E5%9D%97">1. crawler模块</a></li>
<li><a href="#2-item%E6%BA%90%E7%A0%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%AD%98%E5%82%A8key%E5%AD%97%E6%AE%B5">2. item源码如何实现存储key字段,</a></li>
<li><a href="#5-%E5%A4%9A%E4%B8%AAspider%E4%BB%A5%E5%8F%8A%E5%A4%9A%E4%B8%AApipeline%E5%AF%B9%E5%BA%94%E7%9A%84%E8%AE%BE%E7%BD%AE">5. 多个Spider以及多个Pipeline对应的设置</a></li>
<li><a href="#6-%E5%A4%A7%E4%BD%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">6. 大佬的源码解析</a></li>
<li><a href="#7-%E8%AE%BF%E9%97%AEsetting">7. 访问setting</a></li>
<li><a href="#8-%E4%B8%BA%E4%BB%80%E4%B9%88from_crawler%E6%96%B9%E6%B3%95%E9%9C%80%E8%A6%81%E8%AE%BE%E7%BD%AE%E4%B8%BA%E7%B1%BB%E6%96%B9%E6%B3%95">8. 为什么from_crawler方法需要设置为类方法？</a>
<ul>
<li><a href="#9-%E8%AE%BE%E7%BD%AEcookies">9. 设置cookies</a></li>
<li><a href="#10-%E5%85%B3%E9%97%ADscrapy%E6%97%A5%E5%BF%97">10. 关闭scrapy日志</a></li>
<li><a href="#11-%E5%AE%9A%E6%97%B6%E8%B0%83%E5%BA%A6scrapy">11. 定时调度scrapy</a></li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="http://localhost:4000/post/scrapy-item-fu-zhi-tian-chong-xi-jie-zhu-yi/">
              <h3 class="post-title">
                scrapy item赋值/填充 细节注意
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="http://localhost:4000/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
